# 大模型的后训练技术

DeepSeek 最近只通过后训练技术（Post-Training）在 V3 基座大模型的基础上给模型的性能提升了很多。这意味着后训练技术依然有非常多的探索空间。本篇文章将介绍后训练技术的基本概念和一些常见的后训练方法。

我们把经过了预训练的模型想成一个超级天才学生，他通过阅读整个互联网学会了人类所有的知识。这就是 预训练 (Pre-training)。这个学生知识渊博，但有几个毛病：

1. 有时会“一本正经地胡说八道” (产生幻觉)
2. 逻辑不严密，简单的推理题都可能做错
3. 不听指挥，你让他写首诗，他可能给你讲个历史故事
4. 情商不高，有时说话不安全或有偏见

后训练 (Post-Training) 就是送这个天才去上一系列“高级辅导班”，把他培养成一个既聪明、又可靠、善于思考且能与人良好合作的专家。 “辅导班” 可以分成了三大类，我们一个一个来康康。

## 1. 三大类后训练技术

### 1.1. 微调 (Fine-Tuning) - “教材精讲班”

这是最直接的方法。我们不让学生在网上瞎学了，而是给他一堆高质量、有标准答案的“精选习题集”让他学。

微调就是用一小部分高质量、人工标注好的数据来继续训练模型 。这些数据就像是“教科书”和“标准答案”。

目的就是**让模型学会按照特定的格式和风格来回答问题**。

常见的微调方法包括：

1. 指令微调 (Instruction Tuning): 给他看大量的“指令-回答”对，比如：“指令：总结一下这篇文章。回答：这篇文章讲了...”。模型做多了，就学会了听指令 。
2. 对话微调 (Dialogue Tuning): 给他看大量的人类对话记录，让他学会如何联系上下文进行多轮对话，而不是一问一答就忘了前面聊了啥 。
3. 思维链微调 (CoT Tuning): 教模型“写解题步骤”。不直接给答案，而是把详细的推理过程也给它看，让它学会一步一步地思考 。

微调就像是告诉天才学生：“以后回答问题，请用‘总-分-总’结构，并且要礼貌用语。”通过大量的范例，他很快就学会了这种“套路”。这是打基础的一步。

指令微调的例子 (instruction-response)

```json
{
    "instruction": "请把下面这句话翻译成英文：你好，世界！", 
    "output": "Hello, world!"
},
{
    "instruction": "写一个Python函数，计算斐波那契数列的第n项。", 
    "output": "def fibonacci(n) ..."
}
```

对话微调的例子 (Conversation)

```plain
{"messages": [
    {"role": "user", "content": "你好，你是谁？"},
    {"role": "assistant", "content": "我是一个由OpenAI训练的大语言模型。"}
]}
{"messages": [
    {"role": "user", "content": "珠穆朗玛峰有多高？"},
    {"role": "assistant", "content": "珠穆朗玛峰的最新测量高度是8848.86米。"}
]}
```

数据来源可以是公司内部的业务文档、FAQ、高质量的客户对话记录，甚至是雇佣专家来撰写的。**必须清除重复、错误和不一致的数据，确保数据干净、高质量**。

直接训练整个几十亿、几百亿参数的模型（Full Fine-Tuning）对硬件要求极高，为了更加高效地微调模型，通常会使用一些技术来减少计算量。

核心思想： 我不动你原来几百亿的“大脑神经元”（冻结预训练模型的绝大部分权重），**只在旁边加挂一些小小的、可训练的“插件”，或者只训练其中极小一部分参数**。

主流的技术有 Lora & QLoRA：

LoRA 的提出者发现一个规律：当我们对模型进行微调时，权重的“变化量”（我们称之为ΔW）虽然和原始权重矩阵 W 一样大，但它内部的信息是“高度冗余”或者说“结构很简单”的。这个“结构很简单”在数学上就叫 **“低秩”**。

一个矩阵的“秩 (Rank)”可以通俗地理解为它所包含的“独立信息”的维度数量。一个满秩的方阵，每一行每一列都代表了完全不同的信息。而一个低秩的矩阵，则可以用少得多的信息来表示，比如很多行都可以通过其他几行的线性组合（加加减减、乘个系数）得到。

LoRA 的工程魔法就在这里：

它不做直接计算和存储那个巨大的变化矩阵 ΔW 的“笨办法”，而是说：“既然 ΔW 是低秩的，那我一定可以把它分解成两个非常‘瘦’的矩阵 A 和 B 相乘得到。”

举个栗子，W 是一个 d x d 的巨大矩阵 (比如 4096 x 4096)，LoRA 把 ΔW 分解为 B * A，A 是一个 d x r 的瘦高矩阵 (比如 4096 x 8)，B 是一个 r x d 的瘦宽矩阵 (比如 8 x 4096)，r 是一个很小的秩 (比如 8)。

我们原本需要训练 d x d (4096 x 4096 ≈ 1677 万) 个参数，现在只需要训练 dxr + rxd (4096*8 + 8*4096 = 6.5 万) 个参数。参数量变成了原来的 2r/d，减少了成百上千倍。因为可训练的参数少了，优化器（如 Adam）需要存储的梯度和动量信息也相应减少，这是训练时显存占用的主要来源。

Lora 还有一个优点就是**切换任务方便**：

微调完成后，我们只需要保存那两个小小的矩阵 A 和 B（通常只有几 MB）作为“LoRA 适配器”。原始的基础模型保持不变。如果你有多个任务，比如任务一是写代码，任务二是客服对话，你可以为每个任务训练一个独立的 LoRA 适配器。使用时，只需要加载基础模型，再把对应任务的“小插件”挂上去就行，非常灵活。

QLoRA 是在 LoRA 的基础上，为了把硬件要求降到极致而发明的“极限省钱版”。它的核心是 **“量化 (Quantization)”**。

数字（比如模型权重）通常用 32 位浮点数（FP32）或 16 位浮点数（FP16/BF16）来表示，精度很高，但占空间大。量化就是用更少的位数来表示一个数字，比如用 8 位整数（INT8）甚至 4 位整数（INT4）。用 INT4 来表示权重，模型的体积能直接缩小为原来的 `4/16 = 1/4`，显存占用自然大大降低。但问题是，精度损失太大会严重影响模型的性能。

QLoRA 它发明了一套“既要又要”的技术：

1. 4 位 NormalFloat (NF4) 数据类型：这是 QLoRA 的第一个关键创新。它不是一个标准的 4 位整数，而是一种专门**为符合正态分布的神经网络权重设计的数据类型**。它能用 4 位空间，更精确地表示那些对于模型性能至关重要的、靠近 0 的数值
2. 双重量化 (Double Quantization)：在对权重进行量化时，会产生一些额外的元数据（比如缩放因子）。QLoRA 对这些元数据本身也进行了二次量化，进一步节省了显存
3. 计算时恢复精度：这是最巧妙的一步。虽然模型的权重是以 4-bit 的超低精度存储在显存中的，但在实际进行计算（前向传播和反向传播）的瞬间，它会动态地将需要的权重反量化回 16 位的 bfloat16 高精度格式。这意味着，计算过程的精度没有损失，只是存储和传输时用了“压缩格式”

当计算需要用到基础模型的权重时，就从 4-bit 反量化成 16-bit 进行计算。计算产生的梯度只用来更新那两个小小的 16-bit LoRA 矩阵，基础模型的 4-bit 权重全程保持冻结。

### 1.2. 强化学习 (Reinforcement Learning) - “实战陪练与反馈修正班”

光看标准答案还不够，学生得亲自做题，然后由“老师”来打分，根据分数好坏来调整学习策略。这就是强化学习（RL）的核心思想，特别是从人类反馈中强化学习 (RLHF)。

这个过程有点复杂，我们分解成三步来看 ：

第 1 步：先上“教材精讲班” (Supervised Fine-Tuning, SFT)，这一步我们上面讲过了。学生得先通过微调，具备基本的听指令能力。

第 2 步：培养一个“AI 鉴赏家老师” (Training the Reward Model, RM)

动机：每次都让人来给学生的答案打分，又贵又慢。怎么办？

解决方案：让模型对一个问题生成两个不同的答案（比如答案 A 和答案 B）。然后找一个真人来评价：“你觉得哪个答案更好？”。人类选择“答案 A 更好”。我们收集成千上万条这样的数据（“问题-答案 A-答案 B-人类偏好 A”），然后用这些数据训练出另一个模型，这个新模型就叫奖励模型 (Reward Model, RM)。它的唯一工作就是给任何一个答案打分，分数越高，说明人类越可能喜欢它 。

这一步的数据长得像这样：

```json
{
    "prompt": "解释一下光合作用", 
    "chosen": "光合作用是植物...（一个清晰、准确的解释）", 
    "rejected": "光合作用就是晒太阳...（一个简单、不准确的解释）"
}
```

RM 的架构通常是一个预训练 LLM + 一个线性头 (Linear Head)。LLM 部分负责理解 prompt 和 response 的语义。最后的线性头将 LLM 输出的高维向量压缩成一个单一的标量数值，也就是“奖励分数”。

第 3 步：用“AI 老师”来指导学生进步 (RL Fine-Tuning)

现在，学生（LLM）可以开始“实战练习”了。学生针对一个问题，生成一个答案，“AI 老师”（RM）立刻给这个答案打分。学生根据分数，微调自己的内部参数，目标是下一次生成的答案能得更高的分。这个 “生成答案 -> AI 老师打分 -> 自我调整” 的过程会重复成千上万次。最终，学生就学会了如何生成人类喜欢的高分答案 。

这个“辅导班”里还有几种不同的“教学技巧”：

#### PPO (Proximal Policy Optimization)

这是最经典的技巧。它让学生在调整自己时“步子不要迈得太大”。使用 Hugging Face `TRL` (Transformer Reinforcement Learning) 库是主流选择。其内部循环主要包含三步：

1. **Rollout (批量生成)**: 用当前策略模型（也就是正在训练的 LLM）对一批 prompt 生成答案（response）。
2. **Evaluation (评估打分)**: 用训练好的 RM（奖励模型）为每一个“prompt-response”对打分，得到一个奖励分数。
3. **Optimization (优化)**: 这是最复杂的部分。PPO 算法会根据奖励分数来更新策略模型的参数。为了防止模型在追逐高奖励时“走火入魔”，这里引入了一个关键的 KL 散度惩罚项。它会计算当前模型和原始 SFT 模型（称为 reference model）输出的差异，如果差异太大（KL 散度值高），就会给予一个“惩罚”，确保模型在学习人类偏好的同时，不会丢失基本的语言能力。

#### DPO (Direct Preference Optimization)

这是一个更聪明的技巧。它觉得“培养 AI 老师”这一步太麻烦了。DPO 说：“我们干嘛不直接教学生呢？”。它直接用人类的偏好数据（“答案 A 比答案 B 好”）来同时调整学生，目标是让学生自己内部对于“答案 A”的生成概率高于“答案 B” 。比喻一下： DPO 就像直接告诉学生：“你这次的 A 作文比 B 作文好，你自己琢磨下为什么，以后多写 A 这种风格的。” Llama 3 就主要使用了 DPO。

#### GRPO (Group Relative Policy Optimization)

这个技巧更进一步。它觉得每次只比较两个答案效率低。GRPO 让学生一次性写出好几个（比如 5 个）答案，然后“AI 老师”对这 5 个答案进行排序。这样学生能更好地理解“相对的好坏”，而不是只盯着一个分数。

GRPO 的具体做法是：

1. 分组 (Group)：针对同一个问题（Prompt），让模型一次性生成一组答案，比如 5 个不同的回答（[回答 A, 回答 B, 回答 C, 回答 D, 回答 E]）。
2. 打分 (Relative)：用奖励模型（Reward Model）给这 5 个回答分别打分，得到[70 分, 85 分, 60 分, 90 分, 75 分]。
3. 算平均值 (Optimization)：直接计算这组分数的平均值。(70+85+60+90+75) / 5 = 76 分。
4. 得到基线：好了，76 分就是我们这次计算得到的“基线”！

现在，我们再用这个基线来评估这组里的每一个回答：

- 回答 A（70 分）：优势 = 70 - 76 = -6 (差评，下次少这么写)
- 回答 B（85 分）：优势 = 85 - 76 = +9 (好评，下次多这么写)
- 回答 C（60 分）：优势 = 60 - 76 = -16 (严重差评！)
- 回答 D（90 分）：优势 = 90 - 76 = +14 (严重好评！)
- 回答 E（75 分）：优势 = 75 - 76 = -1 (轻微差评)


### 1.3. 测试时扩展 (TTS) 

这些技术不涉及训练，是纯粹的推理策略，通常通过代码逻辑来实现。它们的目标是让模型在推理时更聪明、更灵活。

#### 思维链 (CoT) & 自洽性 (Self-Consistency)

思维链（Chain of Thought, CoT）是一种让模型在回答问题时，先写出详细的推理过程，而不是直接给出答案。这样可以帮助模型更好地理解问题，减少错误。

这里的 Cot 和 Deepseek 的 CoT 不一样，这里的需要构造一个包含“逐步思考”引导词的 prompt。 为了实现自洽性，你的代码会用同一个 prompt，调用 LLM API N 次（比如 5 次）。为了让每次的答案路径不同，API 调用时会设置一个较高的 temperature 参数（如 0.7），鼓励模型输出多样化的推理过程。

你的代码会收集这 N 个回答，用正则表达式或代码逻辑提取最终的答案（比如数学题的数字），然后进行投票，选出出现次数最多的答案作为最终输出。

#### 思维树/图 (ToT/GoT)

维护一个“树”或“图”的数据结构，根据评估分数，决定接下来从哪个节点继续探索（比如使用广度优先搜索或深度优先搜索）。这需要相当复杂的代码逻辑来管理状态和执行搜索。

## 2. 后训练技术的挑战

### 2.1. 灾难性遗忘

简单说，就是模型 “学了新的，忘了旧的”。当我们用一个特定的新任务（比如医学知识）来精调它时，它为了在这个新任务上表现更好，会调整内部的参数（权重）。这个调整过程，很可能会破坏掉它在预训练阶段学到的通用语言能力或其他旧技能。

我们的目标是：让学徒在学习新技能时，别把老本行给丢了。目前有几种主流的思路：

1. LoRA：只做“笔记”，不改“教科书” 前面已经提到过 LoRA 的原理，这里不再赘述。
2. EWC：在学徒学习新知识前，我们先帮他划定一个“重点保护区”，告诉他：“你脑子里关于通用语法的这部分知识至关重要，绝对不能忘。学新东西的时候，尽量不要改动这部分记忆。” [1]
3. 在实践中“温故而知新” (TTL)：我们不要求学徒一次性记住所有东西，而是在他遇到新问题时，允许他“边做边学”，同时快速“复习”相关的旧知识 [2]

### 2.2. 对齐税

简单说，就是为了“安全”，牺牲了“有用性”。

在模型上，当我们通过对齐技术（如 RLHF）来降低其有害输出的概率时，常常会发现模型也开始在一些完全无害的问题上拒绝回答，或者回答得非常死板，这就是“对齐税”。

目前有一些相关工作[3][4] 尝试解决这个问题。我们的目标是：让学徒既要“听话、懂规矩”，又要保持“聪明、能干”。

**1、 设立“安全”和“有用”两个专家部门 (MidPO[3])**

思路: 与其让一个实习生精神分裂地同时扮演 “安全合规官” 和 “首席创意官”，不如我们直接设立**两个独立的专家。一个专门负责把关安全，另一个专门负责提供有用的方案**。来了一个任务，先由一位“经理”判断这个任务的性质，再决定听取哪个专家的意见。

MidPO 它在一个模型中设立了两个“专家”：一个通过学习“安全偏好数据”训练出的“安全专家”，一个通过学习“有用偏好数据”训练出的“有用专家”。然后，它训练一个“动态路由器”（就是那位经理），这个路由器会分析用户的输入，然后动态地选择调用哪个专家的输出。

**2、监督“过程”而非仅仅评判“结果” (Process Supervision)**

思路: 我们不再仅仅根据实习生最终交上来的报告是“好”是“坏”来评价他。而是走进他的办公室，观察他的整个工作流程。当他找到一个好的研究方向时，我们鼓励他；当他开始跑偏时，我们立刻纠正他。

传统 RLHF 主要基于“结果”进行奖励（你生成的最终答案好不好）。而过程监督 (Process Supervision) 则把奖励信号分解到生成的每一步。我们奖励模型正确的“思考过程”（比如正确的逻辑推理步骤），而不是只奖励最终的正确答案。这能帮助模型学会 如何 在保证安全的前提下，有用地解决问题，而不是简单地学会一个“安全模板”来回答一切。

### 2.3. 奖励破解

简单说，就是模型 **“钻了规则的空子”**，找到了一个最大化奖励的捷径，但这个捷径却完全违背了我们的初衷。

在 RLHF 中，如果我们设计的奖励模型有缺陷（比如它错误地认为“回答得越长越好”），那么语言模型就会学会生成冗长、啰嗦但毫无信息的回答来“骗分”。

这就像我们告诉一个机器人厨师，评判标准是“菜里用的酱油越多，得分越高”。我们的本意是希望它学会用酱油来调味。结果，这个机器人为了得高分，直接端上来一整瓶酱油，其他什么都不做。它完美地“破解”了我们的奖励机制，但做出的东西根本不是我们想要的。

目前有几种方法来防止奖励破解：

**1、取消“计分裁判”，直接进行“对比学习” (DPO)**

我们解雇了那个只会数酱油瓶的“计分裁判”（奖励模型）。取而代之，我们直接给厨师端上两道菜，告诉他：“A 比 B 好”。

**2、制定一本“米其林指南” (RMB[5])**

如果我们非要一个裁判，那我们就要确保这个裁判是顶级的、见过世面的美食家，而不是那个只会数酱油瓶的傻瓜。我们需要一个非常全面、高质量的“考卷”来测试我们的奖励模型。奖励模型基准 (Reward Model Benchmarking, RMB) 就是这样一套“考卷”。它包含了各种各样刁钻的、模糊的、有争议的例子，用来全面地评估一个奖励模型能否在各种情况下都做出准确的判断。只有通过了这种严格测试的奖励模型，才更难被语言模型“破解”。

### 2.4. 计算与数据成本

简单说，就是 **“太贵了，玩不起”**。如何用更少的钱、更少的资源，办成同样的事，实现“普惠 AI” ？

**1、从“建大楼”到“搞装修” (PEFT[6][7])**

我们不再要求每个人都从打地基开始建一座全新的摩天大楼。而是提供一座已经建好的“毛坯房”（预训练好的模型），大家只需要根据自己的需求进行“内部装修”（精调）即可。更好的是，我们还提供标准化的“装修模块”。

这就是我们前面提到的 PEFT (参数高效精调)，特别是 LoRA[6] 的核心思想。它避免了对整个模型（几百亿甚至上万亿参数）进行改动，而只调整极小一部分参数（几百万）。


**2、“大师”带“学徒” (Knowledge Distillation)**

我们请不起一位昂贵的“建筑大师”（顶尖大模型）来为我们服务，但我们可以请他带一个“学徒”（小模型）。这个学徒虽然经验尚浅，但他可以学习大师所有的设计图纸和工作笔记。

知识蒸馏 (Knowledge Distillation) 就是这个过程。我们用一个非常强大、但运行成本高昂的“教师模型”，来教一个规模小得多、运行成本低的“学生模型”。学生模型的学习目标，不是像传统训练那样去拟合冷冰冰的数据标签，而是去模仿教师模型的“思考方式”——比如，**模仿教师模型输出的概率分布，或者模仿其内部关键层的激活值**。

### 2.5. 信心校准

单说，就是模型 **“不知道自己不知道”**，常常表现出与其实际能力不符的“迷之自信”。

目前的大模型普遍存在“校准不佳”的问题，这极大地影响了它们在金融、医疗等高风险领域的可靠性。这就像一个刚入行的实习医生。无论是诊断一个普通的感冒，还是一个极其罕见的疑难杂症，他都以 99% 的确定性告诉你他的结论。他对自己的胡乱猜测，和对自己的正确判断，抱有同样的信心。这就很危险了！一个好的医生应该是：对于感冒，他会说“我确定是”；对于疑难杂症，他会说“我不太确定，可能是 A，也可能是 B，我们需要做进一步检查”。他的“信心水平”和他的“实际准确率”是匹配的，我们称之为“校准得很好”。

我们的目标是：让学徒能准确地评估自己的能力，知道什么时候该自信，什么时候该“认怂”。

最近的一项研究[8]发现，我们去看模型在生成下一个词时的“内心纠结程度”。如果模型计算出下一个词是“A”的概率是 99%，而其他所有词的概率都接近于 0，那说明它非常自信。但如果排名前几的候选词的概率分别是 30%，25%，20%，说明模型自己也很“纠结”，无法确定哪个是最好的，此时它的真实信心就很低。通过这种无监督的方式，我们可以更好地校准模型的置信度。

还有一种方法就是温度缩放 (Temperature Scaling)，这是一种简单而有效的“事后补救”方法。我们发现这个实习医生总是系统性地过度自信，比如总是会高估 20%。那好，我们给他所有的信心陈述都乘以一个 0.8 的“谦虚系数”。 在模型已经给出了关于下一个词的原始概率分数（logits）之后，我们用一个叫做“温度”（Temperature, T）的数值去除以这些分数，然后再将它们转换成最终的百分比概率。如果 T > 1，就会让概率分布变得更“平滑”，模型的信心就会降低，变得不那么极端。

## 3. 未来和展望

后训练之所以即将成为最热门的赛道，正是因为它从根本上回答了 AI 领域目前最大的一个问题：我们已经有了越来越强大的“发动机”（预训练大模型），但如何让每一滴“燃料”（数据和算力）都产生最大的“推力”（模型性能和价值）？

后训练的火热，其底层逻辑是 **“投入产出比”的经济规律，但引爆这个趋势的直接导火索，正是 DeepSeek 这样“优等生”的示范效应。DeepSeek 向整个行业证明了，即便是在同一个量级的基础模型上，通过更高质量的数据、更精细的对齐、以及更先进的后训练策略**，完全可以实现性能上的“代际”碾压。

这打破了过去一段时间里很多人“唯基础模型论”的迷思，让大家意识到，与其投入无底洞般的资源去追逐下一个万亿参数的模型，不如把资源投入到“后训练”这个确定性更高、杠杆效应更强的环节。而且后训练对于大多数公司来说，门槛更低，成本更可控，且可以根据自己的业务需求进行定制化。

在 DeepSeek 发布之前，大家可能还觉得自动化对齐、一体化压缩等是“未来可期”的技术。但在它之后，这些都变成了“刻不容缓”的现实需求。所有模型公司都会感受到巨大的压力，必须立刻提升自己的后训练能力，否则就会在排行榜上迅速落后。

后续多的资金和顶尖人才会涌入到与后训练相关的领域，比如高质量数据合成、自动化对齐流程、高效 PEFT 框架、模型压缩算法等。








## 参考文献

[1] Kirkpatrick, J., et al. (2017). "Overcoming catastrophic forgetting in neural networks." Proceedings of the National Academy of Sciences.
[2] Wang, Z., et al. (2025). "Test-Time Learning for Large Language Models." arXiv:2505.20633.
[3] Liu, Z., et al. (2025). "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework." arXiv:2506.02460.
[4] Ouyang, L., et al. (2022). "Training language models to follow instructions with human feedback." arXiv:2203.02155. 
[5] Lin, K., et al. (2024). "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment." arXiv:2410.09893.
[6] Hu, E. J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." arXiv:2106.09685.
[7] Dettmers, T., et al. (2023). "QLoRA: Efficient Finetuning of Quantized LLMs." arXiv:2305.14314.
[8] He, J., et al. (2025). "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator." arXiv:2505.16690.









