# 训练过程中的 GPU 通信

在上一篇文章中，我们从整体上介绍了 Megatron-LM 的预训练流程，从初始化、模型构建到数据加载，再到训练的核心步骤，带你对这个复杂的框架有了初步了解。

随着模型规模的增长，计算资源的增加并不总是能带来线性加速，通信开销成为了训练效率的最大瓶颈之一。如果 GPU 之间的通信无法高效进行，即便计算能力再强，也可能因为等待数据传输而陷入低效运行。

无论是数据并行中的梯度同步，还是张量并行与流水线并行中的计算分布，所有这些策略最终都绕不开一个关键问题：**GPU 之间如何高效通信**？

在这篇文章中，我们将深入探讨大规模分布式训练中的通信机制，特别是 Megatron-LM 这样的大模型训练框架如何利用高效的通信策略来提升训练吞吐量。带着以下问题，阅读本篇文章：

1. GPU 之间是如何交换数据的？
2. Megatron-LM 使用了哪些通信后端（NCCL、Gloo、MPI）？如何选择？
3. 通信模式（AllReduce、AllGather、Send/Recv）如何影响训练效率？
4. 有哪些常见的通信瓶颈？如何优化通信性能？

